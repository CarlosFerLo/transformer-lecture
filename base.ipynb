{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers in Pytorch\n",
    "\n",
    "The idea of this notebook is to explain how transformers are coded in pytorch. We will take as reference the original [Attention is all you need](https://arxiv.org/abs/1706.03762) paper and this [video](https://www.youtube.com/watch?v=ISNdQcPhsts). The transformer we are going to build is rather simple and it will translate sentences from English to Spanish.\n",
    "\n",
    "First we will build the transformer component by component and then we will work on the training loop and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "In order to build the transformer, we will have to build all the inner components first. The base components of the transformer are:\n",
    "- Input Embeddings\n",
    "- Positional Encoding\n",
    "- Layer Normalization\n",
    "- Feed Forward Block\n",
    "- Multi Head Attention Block\n",
    "\n",
    "Then we have the encoder and the decoder, both composed of many encoder and decoder blocks. And finally a projection layer.\n",
    "\n",
    "![Transformer Architecture](assets/transformer-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "This layer will assign a vector to each of the tokens of the input sequence. This vectors are learned during training and represent the \"meaning\" of the token (or word). \n",
    "\n",
    "A `nn.Module` with this functionality already exists in PyTorch, but we will build a module on top in order to make reference to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding (nn.Module) :\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None :\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward (self, x) :\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # as specified in the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "The Positional Encodings adds some vectors to the embeddings in order to encode the position of the token in the sentence (e.g. first, second, ...). There are many ways to archive this, but here we will use the vectors proposed in the original paper, calculated with the following functions:\n",
    "\n",
    "![Positional Encodings Functions](assets/positional-encoding-functions.png)\n",
    "\n",
    "Where $pos$ is the position of the token in the sentence and $i$ is the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module) :\n",
    "    \n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model) # (seq_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(1000) / d_model)) # more numerically stable\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe) # Save it to the state file, but not as a parameter\n",
    "        \n",
    "    def forward (self, x) :\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "This component normalizes each input (each vector corresponding to a token) so its values have mean 0 and variance 1. Then it scales the values with a parameter $\\alpha$ and shifts them with a parameter $\\beta$.\n",
    "\n",
    "The propose of this block is to stabilize and accelerate the training of the model as inputs of the next block will be on a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module) :\n",
    "    \n",
    "    def __init__(self, eps: float = 10**-6) -> None :\n",
    "        super().__init__()\n",
    "        self. eps = eps # numerical stability\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # Multiplied \n",
    "        self.beta = nn.Parameter(torch.ones(1)) # Added\n",
    "        \n",
    "    def forward (self, x) :\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Block\n",
    "\n",
    "This block is a simple fully-connected two layer neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock (nn.Module) :\n",
    "    \n",
    "    def __init__ (self, d_model: int, d_ff: int, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward (self, x) :\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Block\n",
    "\n",
    "This block is the game changer in transformers, its objective is to update the embeddings by giving them some context of the other tokens in the sentence. We use multiple heads in order to focus on different parts of the embeddings in each one, allowing to process different traits and aspects of each word.\n",
    "\n",
    "This implementation will cover both the self-attention, masked-attention and cross-attention, as it changes only the input values and the use of a mask, that will come handy in all three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module) :\n",
    "    \n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\" \n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout) :\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @ value)\n",
    "    \n",
    "    def forward (self, q, k, v, mask) :\n",
    "        query = self.w_q(q) \n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection\n",
    "\n",
    "This last block will be used to handle the residual connection that appears in the diagram, also applying Layer Normalization. This way it gets more compact than simply writing a complex forward function in the encoder and decoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module) :\n",
    "    def __init__(self, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, sublayer) :\n",
    "        return self.dropout(self.norm(x + sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "An encoder block processes the encoder inputs and generates new embeddings that will be later processed by either another encoder block or the decoder.\n",
    "\n",
    "Normally many encoder blocks are present inside the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock (nn.Module) :\n",
    "    \n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ ResidualConnection(dropout) for _ in range(2) ])\n",
    "        \n",
    "    def forward(self, x, src_mask) :\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Now an encoder module that contains a list of Encoder Blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (nn.Module) :\n",
    "    def __init__ (self, layers: nn.ModuleList) -> None :\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask) :\n",
    "        for layer in self.layers :\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "The Decoder Block generates new embeddings for the decoder input tokens based both on masked self attention and cross-attention with the encoder output (attention with the tokens of the encoder input).\n",
    "\n",
    "Normally more than one decoder block is present in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock (nn.Module) :\n",
    "    \n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None :\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ ResidualConnection(dropout) for _ in range(3) ])\n",
    "        \n",
    "    def forward (self, x, encoder_output, src_mask, tgt_mask) :\n",
    "        x = self.residual_connections[0](x, lambda x : self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Now the decoder module that contains all the decoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (nn.Module) :\n",
    "    \n",
    "    def __init__ (self, layers: nn.ModuleList) -> None :\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward (self, x, encoder_output, src_mask, tgt_mask) :\n",
    "        for layer in self.layers :\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Layer\n",
    "\n",
    "This layer will project the embeddings from the decoder output and transform them into probabilities of picking a specific token in each position. It consist of a linear layer followed by a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer (nn.Module) :\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None :\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "Finally everything comes together in the transformer module. We won't build a forward method as we want to be able to run each part separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer (nn.Module) :\n",
    "    \n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        \n",
    "    def encode (self, src, src_mask) :\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode (self, encoder_output, src_mask, tgt, tgt_mask) :\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project (self, x) :\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer (src_vocab_size: int, tgt_vocab_size: int,\n",
    "                       src_seq_len: int, tgt_seq_len: int,\n",
    "                       d_model: int = 512, N: int = 6, h: int = 8, d_ff: int = 2048,\n",
    "                       dropout: float = 0.1) -> None :\n",
    "    \n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbedding(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the positional encoding layers (redundant as we just need one)\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N) :\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "        \n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N) :\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decocer_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decocer_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "        \n",
    "    # Create the encoder and the decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters (Make training faster)\n",
    "    for p in transformer.parameters() :\n",
    "        if p.dim() > 1 :\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To be able to use our model we first need to define a tokenizer, it is a function that will split our text into tokens that appear in our vocabulary. In order to build such vocabulary we will need to train the tokenizer in our dataset.\n",
    "\n",
    "We will use HuggingFace Tokenizers library to build our tokenizer. We will use a word level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from  tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences (ds, lang) :\n",
    "    for item in ds :\n",
    "        yield item[\"translation\"][lang]\n",
    "\n",
    "def get_or_build_tokenizer (config, ds, lang) :\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path) :\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else :\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "        \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are going to use the [Opus Books](https://huggingface.co/datasets/Helsinki-NLP/opus_books), that contains quotes from books in many languages. Our translation model will be from english to spanish. \n",
    "\n",
    "To load the dataset we will use HuggingFace's datasets library, and will use the Dataset and DataLoader classes from Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosfernandezloran/Desktop/transformer-lecture/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the class for our dataset, we expect it to transform our original dataset into tensors we can feed the model, and also to give us the masks to use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset (Dataset) :\n",
    "    \n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        \n",
    "    def __len__ (self) :\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "        \n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids \n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids \n",
    "        \n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "        \n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0 :\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "        \n",
    "        # Add SOS, EOS and PAD to source text\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "        ])\n",
    "        \n",
    "        # Add SOS and PAD to the decoder input\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "        ])\n",
    "        \n",
    "        # Add EOS to the label (What we expect as output from the decoder)\n",
    "        label = torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "        ])\n",
    "        \n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, # (seq_len)\n",
    "            \"decoder_input\": decoder_input, # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, 1, seq_len) & (1, seq_len, seq_len)\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "        }\n",
    "        \n",
    "def causal_mask (size) :\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the function that will do the following:\n",
    "- Load the dataset from HuggingFace\n",
    "- Create or load the tokenizers\n",
    "- Split between train and validation (using random_split from torch)\n",
    "- Build the dataset and dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds (config) :\n",
    "    \n",
    "    # Load the raw dataset\n",
    "    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "    \n",
    "    # Build the tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "    \n",
    "    # Split between training and validation sets\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    \n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "    \n",
    "    # Create dataset objects\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n",
    "    \n",
    "    # Build the dataloaders\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We have been using the configuration parameter for a while, now its time to define it. Feel free to change the configurations to make it work on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config() :\n",
    "    return {\n",
    "        \"batch_size\": 4,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"es\",\n",
    "        \"model_folder\": \"tmodel_\",\n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "    \n",
    "def get_weights_file_path(config, epoch: str) :\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Decoding\n",
    "\n",
    "(First see the [Training Loop](#training-loop)) Here we will define the inference function of the model, needed to run validation during training.\n",
    "\n",
    "We will run the encoder only once, then for every new token on the decoder we will take the previous output with the encoder output to produce the next token. The process ends either with the \\[EOS\\] token or by arriving to max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode (model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device) :\n",
    "    sos_idx = tokenizer_src.token_to_id(\"[SOS]\")\n",
    "    eos_idx = tokenizer_src.token_to_id(\"[EOS]\")\n",
    "    \n",
    "    # Precompute the encoder output and reuse it for every token\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    \n",
    "    # Initialize the decoder input with the SOS token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    while True :\n",
    "        if decoder_input.size(1) == max_len :\n",
    "            break\n",
    "        \n",
    "        # Build the mask for the target (decoder input)\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        \n",
    "        # Calculate the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Get the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        \n",
    "        # Select the token with the max probability (greedy search)\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        \n",
    "        decoder_input = torch.cat([\n",
    "            decoder_input,\n",
    "            torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)\n",
    "        ], dim=1)\n",
    "        \n",
    "        if next_word == eos_idx :\n",
    "            break\n",
    "        \n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "This validation loop consist on running inference through some of the sentences of the validation set and qualitatively asses how good the translations are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation (model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2) :\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    # Size of the control window (default value)\n",
    "    console_width = 80\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "            \n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be one for validation\"\n",
    "            \n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "            \n",
    "            # Print to the console\n",
    "            print_msg(\"-\"*console_width)\n",
    "            print_msg(f\"SOURCE: {source_text}\")\n",
    "            print_msg(f\"TARGET: {target_text}\")\n",
    "            print_msg(f\"PREDICTED: {model_out_text}\")\n",
    "            \n",
    "            if count == num_examples :\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now we will create the function that will train our model. We will be using CUDA as an accelerator if it is available to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len) :\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], config[\"d_model\"])\n",
    "    return model\n",
    "\n",
    "def train_model (config) :\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()) # TODO: find out if .to(device) is needed\n",
    "    \n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
    "    \n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    if config[\"preload\"] :\n",
    "        model_filename = get_weights_file_path(config, config[\"preload\"])\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state[\"epoch\"] + 1\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        global_step = state[\"global_step\"]\n",
    "        \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n",
    "    \n",
    "    for epoch in range(initial_epoch, config[\"num_epochs\"]) :\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator :\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (batch, seq_len)\n",
    "            decoder_input = batch[\"decoder_input\"].to(device) # (batch, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (batch, 1, 1, seq_len) \n",
    "            decoder_mask = batch[\"decoder_mask\"].to(device) # (batch, 1, seq_len, seq_len)\n",
    "            \n",
    "            # Run the tensors through the transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (batch, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)\n",
    "            projection_output = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)\n",
    "            \n",
    "            label = batch[\"label\"].to(device) # (batch, seq_len)\n",
    "            \n",
    "            # (batch, seq_len, tgt_vocab_size) --> (B * seq_len, tgt_vocab_size)\n",
    "            loss = loss_fn(projection_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            \n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            \n",
    "            # Log the loss\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), global_step)\n",
    "            \n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step\n",
    "        }, model_filename)\n",
    "        \n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device, lambda msg: batch_iterator.msg(msg), global_step, writer)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
