# Lecture on Transformers

## Repo Structure

This repo is designed to give a first introduction to coding transformer models.

The [THEORY.md](./THEORY.md) file contains a brief explanation of the mathematical concepts behind the transformer, and [base.ipynb](./base.ipynb) is a code notebook explaining how to code, train and inference a transformer.

The weights folder contains training checkpoints of the model.

## Setup

1. Set up your python environment, follow this [guide](https://www.hostinger.com/tutorials/how-to-create-a-python-virtual-environment?gad_campaignid=20592453563&gbraid=0AAAAADMy-hb6kX1z27btePaLrM3NdyqOz).

2. Install the project dependencies by running: `pip install -r requirements.txt`

And you are good to go!

## References

- [All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 1](https://medium.com/data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021)
- [All you need to know about ‘Attention’ and ‘Transformers’ — In-depth Understanding — Part 2](https://medium.com/data-science/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada)
- [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA)
- [Build your own Transformer from scratch using Pytorch](https://medium.com/data-science/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb)
